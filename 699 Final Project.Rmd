---
title: "Project"
author: "Team"
date: "5/4/2020"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE, message=FALSE)
```

## Step I: Data Preparation & Exploration (15 points)
Read your data into your local environment, and subset/filter the data so that you are dealing only with the records that pertain to your team’s neighborhood group. If you use read_csv() you will be reading the dataset in with Unicode -- this may help out with the rendering of some of the German-specific characters.
```{r echo = T, results = 'hide' }
library(dplyr)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(leaflet)
library(wordcloud)
library(gutenbergr)
library(tidytext)
library(scales)
library(stringr)
library(reshape2)
library(car)
library(forecast)
library(FNN)
library(caret)
library(e1071)
library(rpart)
library(rpart.plot)
```

```{r, results = 'hide' }
listings_summary = read.csv("listings_summary.csv",na.strings=c("","none"))
listings_summary_2 = read.csv("listings_summary.csv",na.strings=c("","none","N/A"),stringsAsFactors = F)
PB_data = listings_summary[listings_summary$neighbourhood == "Prenzlauer Berg",]
PB_data_2 = listings_summary_2[listings_summary_2$neighbourhood == "Prenzlauer Berg",]
dim(PB_data)
head(PB_data)
View(PB_data)
```

```{r, results = 'hide'}
# select the meaningless variables
# scrape_id
nomeaning = c("listing_url","scrape_id","space",
              "thumbnail_url","medium_url","picture_url","xl_picture_url","host_url","host_thumbnail_url","host_picture_url")
PB = select(PB_data,-nomeaning)
PB.2 = select(PB_data_2,-nomeaning)
# View(PB)
```

```{r, results = 'hide'}
dim(PB_2)
```



### I. Missing Values
#### A. Does your data contain any missing values and/or blank cells? If so, what can you do about this? Show the R code that you used to handle your missing values.

```{r}
# str(PB)
map(PB,~sum(is.na(.)))  # amount of na in specific columns
```
Yes. There are a lot of missing values contained in the dataset. It presents as NA and blank cells. 

```{r, results = 'hide'}
# delete 1131 rows 
missrows = PB[is.na(PB$id) == T,]   # find rows that id = na
sum(!is.na(missrows))
PB_2 = PB[is.na(PB$id) == F,]      # delete rows that id = na
PB_3 = PB.2[is.na(PB.2$id) == F,]      # delete rows that id = na

```

```{r, results = 'hide'}
PB_2[PB_2=="N/A"]<-NA
map(PB_2,~(sum(is.na(.))/nrow(PB_2)))    # Percentage of NA in the cleaned dataset


#c("experiences_offered","host_acceptance_rate", "jurisdiction_names")  # All info are NAs
PB_2$experiences_offered <- NULL
PB_2$host_acceptance_rate <- NULL
PB_2$jurisdiction_names <- NULL

# Change host response rate to different levels: High, Low, NA
PB_2$host_response_rate = as.numeric(sub("%","",PB_2$host_response_rate))
PB_2$host_response_rate[PB_2$host_response_rate >= 70]<- "High"
PB_2$host_response_rate[PB_2$host_response_rate < 70]<- "Low"
#view(PB_2)

```




#### B. Write one paragraph describing what you did, and why you did it. (Note: You may wish to deal with missing values differently for different tasks. You are not ‘locked in’ to a decision regarding missing values).

First, we delete some columns that may not be meaningful for analysis or later model building, such as URLs for different sources. Then, we deleted rows that “id” is  NA which there’s no information for those rows. After we viewed the dataset, we found there are some NA values shown as N/A. Thus, we convert the N/As , which is not the standard way to show blank, into NAs. And then use map() function to see the percentage of NAs in each column. The column with 100% of NA should be deleted from the data set. In our neighborhood, columns containing all NAs are experiences_offered, host_acceptance_rate, jurisdiction_names. We keep NA values in the factor columns since those may have analysis meanings for later use; and we also keep some NA values in the numeric column at the beginning, and we will change those differently based on different model building later.



#### C. Data Preparation for Prediction and Clustering
```{r, results = 'hide'}
allNAcol = c("experiences_offered","host_acceptance_rate", "jurisdiction_names")
PB_3 = select(PB_3,-allNAcol)
# find mode in dataset
FindMode <- function(x) {
    ux <- unique(x)
    ux[which.max(tabulate(match(x, ux)))]
}

# host_location mode value is "Berlin, Berlin, Germany" 
PB_3$host_location[is.na(PB_3$host_location)] <- FindMode(PB_3$host_location)


# transform missing value to "N/A"(not available). create a level for null value
# host_response_time N/A
PB_3$host_response_time[is.na(PB_3$host_response_time)] <- "N/A"

# host_response_rate {high: host_response_rate >= 80%}, {low: host_response_rate < 80%}, {N/A: not available}
PB_3$host_response_rate <- as.numeric(sub("%","",PB_3$host_response_rate))/100
summary(as.factor(PB_3$host_response_rate))
PB_3$host_response_rate[PB_3$host_response_rate>= 0.8] <- "high"
PB_3$host_response_rate[PB_3$host_response_rate< 0.8] <- "low"
PB_3$host_response_rate[is.na(PB_3$host_response_rate)] <- "N/A"
summary(as.factor(PB_3$host_response_rate))

# host_is_superhost NA => f
PB_3$host_is_superhost[is.na(PB_3$host_is_superhost)] <- "f"

# host_neighbourhood {Prenzlauer Berg}, {Other}, {NA => Prenzlauer Berg}
PB_3$host_neighbourhood[PB_3$host_neighbourhood != "Prenzlauer Berg"] <- "Other"
PB_3$host_neighbourhood[is.na(PB_3$host_neighbourhood)] <- "N/A"
summary(as.factor(PB_3$host_neighbourhood))


# host_listings_count  NA=>  mode

PB_3$host_listings_count[is.na(PB_3$host_listings_count)] <-  FindMode(PB_3$host_listings_count)
summary(as.factor(PB_3$host_listings_count))

# host_total_listings_count mode
PB_3$host_total_listings_count[is.na(PB_3$host_total_listings_count)] <-  FindMode(PB_3$host_total_listings_count)
summary(as.factor(PB_3$host_total_listings_count))


# host_has_profile_pic mode
PB_3$host_has_profile_pic[is.na(PB_3$host_has_profile_pic)] <-  FindMode(PB_3$host_has_profile_pic)
summary(as.factor(PB_3$host_has_profile_pic))



# host_identity_verified mode
PB_3$host_identity_verified[is.na(PB_3$host_identity_verified)] <-  FindMode(PB_3$host_identity_verified)
summary(as.factor(PB_3$host_identity_verified))



# city, state, zipcode
summary(as.factor(PB_3$city))  
PB_3$city = "Berlin"           # All city should be Berlin
summary(as.factor(PB_3$state))  
PB_3$state = "Berlin"           # All state should be Berlin

summary(as.factor(PB_3$zipcode))  
PB_3$zipcode[PB_3$zipcode == "10119\n10119"] = "10119" # data cleaning
PB_3$zipcode[is.na(PB_3$zipcode)] <- "N/A"            

# smart_location,street
summary(as.factor(PB_3$smart_location))  
PB_3$smart_location = "Berlin, Germany"           # All smart location should be "Berlin, Germany"

summary(as.factor(PB_3$street))  
PB_3$street = "Berlin, Germany"           # All smart location should be "Berlin, Germany"



# market
summary(as.factor(PB_3$market))  
PB_3$market[is.na(PB_3$market)] <-  FindMode(PB_3$market)


# bathrooms, bedrooms, beds NA => mode
summary(as.factor(PB_3$bathrooms))  
PB_3$bathrooms[is.na(PB_3$bathrooms)] <-  FindMode(PB_3$bathrooms)

summary(as.factor(PB_3$bedrooms))  
PB_3$bedrooms[is.na(PB_3$bedrooms)] <-  FindMode(PB_3$bedrooms)

summary(as.factor(PB_3$beds))  
PB_3$beds[is.na(PB_3$beds)] <-  FindMode(PB_3$beds)

# license
summary(as.factor(PB_3$license))  
PB_3$license[which(is.na(PB_3$license)==F)] = "Yes"
PB_3$license[which(is.na(PB_3$license)==T)] = "No"

# Extract year and month.   first_review      last_review    host_since
PB_3$first_review = substring(PB_3$first_review,1,4)
PB_3$last_review = substring(PB_3$last_review,1,4)
PB_3$host_since = substring(PB_3$host_since,1,4)
PB_3$first_review[is.na(PB_3$first_review)] <- "N/A"            
PB_3$last_review[is.na(PB_3$last_review)] <- "N/A"            
PB_3$host_since[is.na(PB_3$host_since)] <- "N/A"  
PB_3$first_review = as.character(PB_3$first_review)
PB_3$last_review = as.character(PB_3$last_review)
PB_3$host_since = as.character(PB_3$host_since)

# 
dollar_cols = c("price","weekly_price","monthly_price","security_deposit","cleaning_fee","extra_people")
for(i in dollar_cols){
    PB_3[i] = sub("^.","",as.vector(t(PB_3[i])))
   PB_3[i] = as.numeric(sub(",","",as.vector(t(PB_3[i]))))
}


for(i in 1:ncol(PB_3)){
    if (mode(PB_3[,i]) == "character"){
        PB_3[,i] = as.factor(PB_3[,i])
    }
}
#summary(PB_3) 
```



### II. Summary Statistics
#### A. Take a peek at your data, and then brainstorm a bit about some questions that you’d like to answer with summary statistics. To answer these questions choose any five of the summary statistics functions shown in the textbook, class slides, or anywhere else to learn a little bit about your data set.
```{r, results = 'hide'}
summary(PB_2)
```

#### B. Show screenshots of the results. Describe your findings in 1-2 paragraphs.
Q1: What is the mean price for apartments in 2018? Compare apartments mean price of 2018 with apartments mean price in all years, which one is lower?
Apartment mean price in 2018 is lower compared with the overall apartment mean price.
```{r}
#str(PB_2)
PB_2$host_since<- as.Date(PB_2$host_since)

Y2018<- filter(PB_2, host_since > "2017-12-31")
AY2018<- filter(Y2018, property_type =="Apartment")
AY2018$price<- as.numeric(AY2018$price)
MA2018<- mean(AY2018$price)
MA2018

AptPAll<-filter(PB_2, property_type =="Apartment")
AptPAll$price<- as.numeric(AptPAll$price)
MeanAptAll<-mean(AptPAll$price)
MeanAptAll

```

Q2: What is the price standard deviation for apartments in 2018? Compare with apartments’ price standard deviation in all years.
```{r}
SdA2018<- sd(AY2018$price)
SdA2018

SdAptAll <- sd(AptPAll$price)
SdAptAll
```


Q3: What is the price range for apartments in 2018? Compare it with the apartment price range in all years.
```{r}
RA2018<- range(AY2018$price)
RA2018

Rall<- range(AptPAll$price)
Rall

```

Q4: What is the percentage of each room type?
```{r}

RTdata <- PB_2 %>% group_by(room_type) %>% count() %>% ungroup() %>% 
  mutate(per=`n`/sum(`n`)) %>% arrange(desc(room_type)) 
RTdata$label <- paste(round(RTdata$per,3)*100,"%",sep = "")
RTdata

```

Q5: What is the quantile for apartments in 2018? Compare it with quantile on all property types in all years.
```{r}
QA2018<- quantile(AY2018$price, c(0.25, 0.75))
QA2018

PB_2$price<- as.numeric(PB_2$price)
Qall<- quantile(PB_2$price, c(0.25, 0.75))
Qall
```



The mean price for apartments in 2018 is \$184.5. Apartments mean price in all years is \$192.34. It indicates apartments’ price in 2018 is relatively lower than other years. Apartments’ price standard deviation in all years is lower than standard deviation in2018. A smaller standard deviation means that the values in a statistical data set are close to the mean of the data set. It indicates more price polarization happened in 2018. It could be that some new hosts joined airbnb in this neighborhood, or some hosts adjusted their prices. Then we used the range() function to see data dispersion on apartments. Price range in 2018 is \$10 to \$294; price range in all years is \$1 to \$294 which includes year 2018’s data. It means there were apartments that reached the highest price in 2018. To have a better understanding of distribution on room types, we grouped by room types and calculated the percentage of each one. The result shows 57.2% hosts offered their entire apartment or house for rent. Quantiles are the lines that divide data into equally sized groups. Results of using quantile functions on apartment prices show the average level of apartment prices in 2018 were between \$160.5 and \$241; and the average level of apartment prices in all years were between \$165 and \$247. The average level of apartment prices were lower than the overall average price. 









### III. Data Visualization
#### A. Using ggplot, create any five plots that help to describe your data. As you do, remember to think about the types of variables that you are representing with a particular plot. Think of these plots a sexpository(notexploratory)so be sure to include clear axis labels and plot titles.
Q1: What’s the property types in the neighborhood? 
```{r}
# Bar plot on property type
ggplot(PB_2, aes(property_type),xlab("Property Type"))+geom_bar(aes(fill=property_type),position = "dodge")+ 
  ggtitle("Counts on Property Types")+theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.5))
```
Q2: Percentage of room types distribution


```{r}
# Percentage of room types distribution shown as Pie Chart
ggplot(data=RTdata)+ geom_bar(aes(x="", y=per, fill=room_type), stat="identity", width = 1)+ 
  coord_polar("y", start=0) + theme_void()+ geom_text(aes(x=1, y = cumsum(per) - per/2, label=label)) + ggtitle("Percentage of Room Types Distribution")

```

Q3: host response rate high low or unknown
```{r}
# Bar chart of Host Response Rate
ggplot(PB_2, aes(host_response_rate),xlab("Host Response Rate"))+geom_bar(aes(fill=host_response_rate),position = "dodge")+ 
  ggtitle("Host Response Rate")+theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5))
```



Q4: host_since in 2018 with apartment price - scatter chart
```{r}
# scatter plot for price (apartment since 2018)
ggplot(AY2018, aes(x=host_since, y=price))+geom_point()+
  ggtitle("Price for Apartment with host since 2018")
```



Q5: Average price in different zip code
```{r}
# Average price for Aribinb in different zip code in neighborhood Prenzlauer Berg
AveZip<- select(PB_2, zipcode, price)
AveZip<-na.omit(AveZip)
AveZip<- AveZip%>%
  group_by(zipcode)%>%
  summarise(average= mean(price))
  
ggplot(AveZip, aes(x=zipcode, y=average))+geom_point()+
  ggtitle("Average price in different zipcode")+
  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5))
```


#### B. Write a two-paragraph description that explains the choices that you made, and what the resulting plots show.

It is a big dataset. Although we only focus on the Prenzlauer Berg neighborhood and cleaned our data, there are still 2768 records. We created a bar chart on property types, the bar chart can clearly show what property types available in this area and numbers for each of them. It shows the majority property type is apartment; condominium rank as the second; loft rank as the third. The pie chart for room types not only can show what room types are available, but also can show the percentage distribution for each type. The chart shows 57.2% room type is renting the entire apartment or house; 42.1% room type is private rooms; 0.8% room type is shared room. Reasons for the high proportion of private rooms are those are better on pricing or renter is self-travelled.   We changed the host response rate from numerical to categorical into two levels which are high and low and keeps NA values as NA at data cleaning period. Response rate over 70 percent regard as high; otherwise it regards as low.  Bar chart on host response rate helps to see the overall host response performance. Most hosts have high response rates. However, There are even more NAs which means unrated. Many people don’t have the habit of giving ratings and reviews. People usually only give ratings when they have superior or extremely bad experience. It means hosts are doing fine on giving responses. But renters’ overall experiences on the property need to be further analyzed with other factors. 

Since 2018 is the most recent year in the dataset and apartments are the majority property type, we made a scatterplot to see the price distribution. From the chart, it shows most apartments’ prices are above \$150.  Prenzlauer Berg has different distinct areas that have different zip codes. The fifth chart shows average rent prices in different zip codes. There are only two areas with an average price under \$150. Most areas pricing are around \$200. It indicates this neighborhood may be around the center of Berlin. 










### IV. Mapping
#### A. Generate a map that depicts the location of the properties in the dataset that are in your neighborhood. On this map, show the various property types using different colors. Do the property types appear to be geographically clustered at all, or are they dispersed in a mostly even way throughout?

```{r}
pal <- colorFactor("RdYlBu",domain = PB_2$property_type)
leaflet(data = PB_2) %>% 
  addCircles(col = ~pal(property_type)) %>% 
  addTiles()
```

The map shows that most of the property type is apartments (dark red). Most Airbnb located in the south-west and north-west of Prenzlauer Berg. All types of houses  are particularly dense in the west near the middle, while fewer in central and west. The distribution of different property types is basically uniform in the area. Proportionately, most apartment houses are closer to the center of the city.

 Because the proportion of apartments is so large, it is difficult to see the distribution of other property types of house. Therefore, we took out the apartment type and redrew a map to talk about the distribution of other types.


```{r}
# A new Map to see the spread of other property types(besides apartment)
No_apt<-filter(PB_2, property_type != "Apartment")
pal2 <- colorFactor("RdYlBu",domain = No_apt$property_type)
leaflet(data = No_apt) %>% 
  addCircles(col = ~pal(property_type)) %>% 
  addTiles()
```

This new map shows the property type of house (dark blue) mostly located on the edge of the city. Beyond that, other types of properties are more evenly distributed.

Overall, in my opinion, the distribution of different types of houses in Prenzlauer Berg does not have obvious geographically clustering features. 








### V. Wordcloud
#### A. Using the neighborhood overview column in your dataset, generate a wordcloud. What are some terms that seem to be emphasized here?
```{r}
overview<- select(PB_2, neighborhood_overview)
overview<-  na.omit(overview)
overview <- data.frame(lapply(overview, as.character), stringsAsFactors=FALSE)

overview <- overview %>%unnest_tokens(word, neighborhood_overview)
overview<- overview %>%anti_join(stop_words)
over_fre<- overview %>%count(word, sort = TRUE)

set.seed(190)
wordcloud(words = over_fre$word, freq = over_fre$n, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

```

As the Word Cloud shows, the most frequent word used in neighborhood overview is “restaurants”. This suggests that people most often refer to nearby restaurants when commenting on the neighborhood. Next, followed by the word “und”-- a stop word in German, it was not filtered out because we did not do shifting with German. But it also reflects that there are many host writing overviews in German or German mixed with English. “Prenzlauer Berg”, and “Berlin” are also hot words. Others are “bars”, “shops”, “minutes”, “cafes” and “mauerpark”. 

The Word Cloud illustrates that when hosts describe the neighborhood, they pay much attention to the restaurants and shops around and whether the area is convenient for access to various dining or entertainment places. 











## Step II: Prediction (20 points)
### I. Create a multiple regression model with the outcome variable price.
### A. Describe your process. How did you wind up including the independent variables that you kept, and discarding the ones that you didn’t keep? In a narrative of at least two paragraphs, discuss your process and your reasoning. In the write-up, be sure to talk about how you evaluated the quality of your model.



#### Data Preparation for Linear Regression Modeling



**1. Missing Value Imputation**

square_feet
```{r}
PB_lm = PB_3
#summary(PB_lm$square_feet)  # there is too many missing value, therefore, I delete it in 3.(2) part.
PB_lm$square_feet[is.na(PB_lm$square_feet) == T] <- "N/A"
PB_lm$square_feet <-as.factor(PB_lm$square_feet)
```

security_deposit
I assumed security_deposit is 0 if there is no information about security deposit
```{r}
#summary(PB_lm$security_deposit)
PB_lm$security_deposit[is.na(PB_lm$security_deposit)] <- 0
```

cleaning_fee. I assumed cleaning_fee. is 0 if there is no information about the fee
```{r}
#summary(PB_lm$cleaning_fee)
PB_lm$cleaning_fee[is.na(PB_lm$cleaning_fee)] <- 0
```

reviews
```{r}
# review_scores_rating, review_scores_accuracy, review_scores_cleanliness, review_scores_checkin, review_scores_communication, review_scores_location,  review_scores_value

# no_review_data = PB_lm[is.na(PB_lm$review_scores_accuracy) == T,]
review_cols = c("review_scores_rating", "review_scores_accuracy", "review_scores_cleanliness", "review_scores_checkin", "review_scores_communication", "review_scores_location",  "review_scores_value")
for (i in review_cols){
        PB_lm[i][is.na(PB_lm[i]) == T] = median(PB_lm[,i],na.rm = T)
}

# reviews_per_month
PB_lm$reviews_per_month[is.na(PB_lm$reviews_per_month) == T] = 0
#summary(PB_lm$reviews_per_month)

PB_lm$first_review  = as.factor(PB_lm$first_review)
#summary(PB_lm)

```



**2. Feature Engineering**
property_type  {Apartment}, {Other}
```{r}
#summary(as.factor(PB_lm$property_type))
PB_lm$property_type[PB_lm$property_type != "Apartment"] <- "Other"
PB_lm$property_type <- droplevels(PB_lm$property_type)
#summary(as.factor(PB_lm$property_type))
```

weekly_price and monthly_price
*In this step, we can not use the exact number from weekly_price and monthly_price. Because the two prices are calculated after we decide the daily price. However, we can measure whether the airbnb provide the discount or not. If providing, we use the value as "offer discount", and if not, the value will be "no discount"*
```{r}
# weekly_price
# offer high discount or not
#summary(PB_lm$weekly_price)
for (i in 1:length(PB_lm$weekly_price)) {
        if (is.na(PB_lm$weekly_price[i]) == T){
                PB_lm$weekly_price[i] = PB_lm$price[i]*7
        }
}

weekly_discount_ratio = (PB_lm$weekly_price)/(PB_lm$price*7)
weekly_discount_ratio[is.na(weekly_discount_ratio) == T] = 1
#summary(weekly_discount_ratio)
for (i in 1:length(weekly_discount_ratio)){
        if (weekly_discount_ratio[i] < 1 ){
                PB_lm$weekly_price[i] = "offer discount"
        }
        else{
                PB_lm$weekly_price[i] = "no discount"
        }
}
PB_lm$weekly_price = as.factor(PB_lm$weekly_price)
#summary(PB_lm$weekly_price)

# monthly_price
#summary(PB_lm$monthly_price)
for (i in 1:length(PB_lm$monthly_price)) {
        if (is.na(PB_lm$monthly_price[i]) == T){
                PB_lm$monthly_price[i] = PB_lm$price[i]*30
        }
}

monthly_discount_ratio = (PB_lm$monthly_price)/(PB_lm$price*30)
monthly_discount_ratio[is.na(monthly_discount_ratio) == T] = 1
#summary(monthly_discount_ratio)
for (i in 1:length(monthly_discount_ratio)){
        if (monthly_discount_ratio[i] < 1 ){
                PB_lm$monthly_price[i] = "offer discount"
        }
        else{
                PB_lm$monthly_price[i] = "no discount"
        }
}
PB_lm$monthly_price = as.factor(PB_lm$monthly_price)
#summary(PB_lm$monthly_price)

```

calendar_updated
We transformed the categorical variables into numeric variables as the day after last updated. For example, "a week ago" will be replaced by 7. 
```{r}

# calendar_updated
#summary(PB_lm$calendar_updated)
calendar_updated = gsub(" ago","",PB_lm$calendar_updated)
calendar_updated = gsub("weeks","week",calendar_updated)
calendar_updated_day = c()
#summary(as.factor(calendar_updated))
# length(PB_lm$calendar_updated)
for(i in 1:length(PB_lm$calendar_updated)){
        if (str_detect(calendar_updated[i],"week") == T){
                number =gsub(" week","" ,calendar_updated[i])
                if (number == "a"){
                        number = "1"
                }
                number = as.numeric(number)*7
        }
        if (str_detect(calendar_updated[i],"months") == T){
                number =gsub(" months","" ,calendar_updated[i])
                number = as.numeric(number)*30
        }
        if (str_detect(calendar_updated[i],"today") == T){
                number =gsub("today","1" ,calendar_updated[i])
                number = as.numeric(number)
        }
        if (str_detect(calendar_updated[i],"yesterday") == T){
                number =gsub("yesterday","2" ,calendar_updated[i])
                number = as.numeric(number)
        }
        if (str_detect(calendar_updated[i],"days") == T){
                number =gsub(" days","" ,calendar_updated[i])
                number = as.numeric(number)
        }        
        if (str_detect(calendar_updated[i],"never") == T){
                number =gsub("never","10000" ,calendar_updated[i])   
                number = as.numeric(number)
        }       # 10000 is a sign, we will replace it by the maximized calendar updated day
        calendar_updated_day = c(calendar_updated_day,number)
        
}
calendar_updated_day[calendar_updated_day == 10000] = calendar_updated_day[max(calendar_updated_day[calendar_updated_day<10000])]
#summary(calendar_updated_day)
PB_lm$calendar_updated = calendar_updated_day

#summary(PB_lm)
```




**3. Feature Selection**

**(1) drop useless variables**
```{r}
PB_lm = subset(PB_lm,select =- c(id,name,summary,description,neighborhood_overview,notes,host_verifications,
                                latitude,longitude,transit,access,interaction,house_rules,
                                host_id,host_name,host_about,amenities))
# summary(PB_lm)
```

**(2) Delect the feature whose primary value ratio over 80%**
When the primary value ratio is over a critical level, the feature has less predictable ability.
```{r , results = 'hide'}
#tmp = PB_lm
RowNumber = nrow(PB_lm)
cols = colnames(PB_lm)
Primary_Value_Ratio = c()
for (i in cols){
        Primary_Value_Ratio = c(Primary_Value_Ratio,max(table(PB_lm[i]))/RowNumber)
}
Primary_Value = data.frame(cols,Primary_Value_Ratio)
Primary_Value[Primary_Value$Primary_Value_Ratio>0.8,]

PB_lm = select(PB_lm,-Primary_Value[Primary_Value$Primary_Value_Ratio>0.8,]$cols)
```


**Spliting Dataset**
*In the following steps, we might use the informatin from validation dataset. For example, we use the dataset to measure the correaltions between each features. To avoid information leakage, I split dataset in this step*
```{r}
set.seed(100)
newPB_lm <- sample_n(PB_lm, n())
bp <- nrow(newPB_lm)*0.6
train_data <- slice(newPB_lm,1:bp)
valid_data <- slice(newPB_lm,(bp+0.0001):nrow(newPB_lm))
```

**(3) High Correlation Detection**
```{r}
cols = colnames(train_data)
numeric_cols = c()
for(i in cols){
        if ((class(train_data[,i]) == "numeric")|(class(train_data[,i]) == "integer")){
            numeric_cols = c(numeric_cols,i)    
        }
}
train_data_Numeric = select(train_data,numeric_cols)
#View(train_data_Numeric)
cormat <- cor(train_data_Numeric)
#head(cormat)



melted_cormat <- melt(cormat)
#head(melted_cormat)
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "lightblue3", high = "salmon", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation") +
  theme_minimal()+ # minimal theme
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 9, hjust = 1))+
 coord_fixed()
# Print the heatmap
print(ggheatmap)
#View(cormat>0.7)
```

3 pairs of variables with high correlation(larger than 0.7):
* **host_total_listings_count** & **host_listings_count** & **calculated_host_listings_count**
* **accommodates** & **beds** 
* **availability_30** & **availability_60** & **availability_90**

```{r, results = 'hide'}
View(cormat[6,])
```


To avoid multicollinearity, to decide to remove the following varibles:**host_listings_count**, **host_total_listings_count**, **beds**, **availability_30** and **availability_60**

```{r}
#tmp_train = train_data
#tmp_valid = valid_data
high_cor_cols = c("host_listings_count","host_total_listings_count","beds","availability_30", "availability_60")
train_data = select(train_data,-high_cor_cols)
valid_data = select(valid_data,-high_cor_cols)
```





**(4) Skewness of Dependent Variable**
```{r}
cols = colnames(train_data)
numeric_cols = c()
for(i in cols){
        if ((class(train_data[,i]) == "numeric")|(class(train_data[,i]) == "integer")){
            numeric_cols = c(numeric_cols,i)    
        }
}
n_row = nrow(train_data)
 # Histogram overlaid with kernel density curve
ggplot(train_data, aes(x=price)) + 
        geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                       binwidth=.5,
                       bins = 10,
                       colour="black", fill="white") +
        geom_density(alpha=.2, fill="#FF6666")  # Overlay with transparent density plot
```
* Price follows right-skewed Distribution. It is a common phenomenon for most real-life variables. 
* However, for linear regression model, it is essential for residual of the model to follow normal distribution. And then the response variable will also follow. 
* Here we use log transformation to make response variable into normal distribution. The new repsonce variable should be log(price). For log transforming, the data point which price is 0 should be deleted. 
* In addition,3 data points seem to be outliers. These prices are over 1000 dollars for one night. We can not deny their existance. However, we should delete them to generalize our model.


```{r}

train_data3 = train_data[(train_data$price != 0)&(train_data$price < 1000),]
train_data3$log_price = log(train_data3$price)
train_data3 = select(train_data3 , -price)
# summary(train_data3$log_price)

ggplot(train_data3, aes(x=log_price)) + 
        geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                       binwidth=.5,
                       bins = 10,
                       colour="black", fill="white") +
        geom_density(alpha=.2, fill="#FF6666")  # Overlay with transparent density plot

```









#### Modeling

**1. Build a initial model**
```{r, results = 'hide'}
# Build the initial model
lm_1 <- lm(log_price ~., data = train_data3)

lendfitback <- step(lm_1,direction = "backward")
summary(lendfitback)
```


**2. Detecting Multicollinearity**
```{r}

vif(lendfitback,digits = 3)
```
VIF values of neighbourhood_cleansed, zipcode, first_review are large, which indicates that there is significant Multicollinearity problem with 3 features. Therefore, we delete them and rebuild a new model.


**3. build a second model w/o 3 multicollinearity features**

```{r, results = 'hide'}
# Build the second model
lm_2 <- lm(log_price ~. - neighbourhood_cleansed - zipcode-first_review, data = train_data3)

lendfitback_2 <- step(lm_2,direction = "backward")
summary(lendfitback_2)
```

**4. Check VIF again**
```{r}
vif(lendfitback_2,digits = 3) #Check again
```


**5. Residual Analysis**

```{r}
plot(lendfitback_2)
```

Residual Plot 1: **Resuduals vs. Fitted**. Even though residuals of some outliers over the range of [-2,2], most of data points randomly distributes around 0. This model fits the data well
Residual Plot 2: **Normal Q_Q Plot**. Some data points deviate from the diagonal, so residuals do not follow normal distribution strictly. 
Residual Plot 3: **Scale-Location**. Seem to be constant with no trend. There is no serious problem in Heteroskedasticity.


Residual Independence Analysis: In Durbin Watson Test, the p-value is larger than 0.05, we can assume errors are independent.
```{r}
durbinWatsonTest(lendfitback_2)
```
In conclusion, the residuals can be regarded as stochastic error.


#### Prediction in valid dataset
```{r}
valid_data$log_price = log(valid_data$price)

# Make predictions
predictions_train <-  predict(lendfitback_2,train_data3)
predictions_valid <-  predict(lendfitback_2,valid_data)
```

```{r}
# Model performance
summary(lendfitback_2)

accuracy(predictions_train,train_data3$log_price)
accuracy(predictions_valid,valid_data$log_price)
```




### B. Show a screenshot of your regression summary, and explain the regression equation that it generated.

1. What is the r-squared of your model? What does this mean?


* The adjusted r-squared is only 0.5277. This means only 52.77% variation has been explained by the multiple regression model. 
* In general, the higher the R-squared, the better the model fits your data. Even though the R-squared is not closing to 1, we also can infer the sample data are well correspond to the fitted (assumed) model. In fact, R-squared doesn’t tell us the entire story. After we check the residual plots, we know the residuals independent and identically distributed in normal distribution. That is, this model fits the data well.


2. What is the RMSE of your model? What does this mean?

* In valid dataset, RMSE is 0.4206808. The standard deviation of the predictions from the actual values in valid dataset would be 0.4206808. 
* What's more, RMSE in training dataset is 0.3775776. The difference between these two number is small. This tells us the model is good-fit and has generalization ability. The model has the ability to adapt properly to new, previously unseen data, drawn from the same distribution as the one used to create the model.







## Step III: Classification (40 points)
### Part I. Using k-nearest neighbors, predict the type of cancellation policy that a rental unit in your neighborhood will have. Use any set of numerical predictors in order to build this model. You do not have to just accept the levels of this factor “asis”--in other words, you may wish to explore with combinations, modifications, etc.
#### A. Show the code you used to run your model, and the code you used to assess your model.
```{r, results = 'hide'}
#Part I: k-nearest neighbors#



#summary(PB_2$cancellation_policy)
str(PB_2$cancellation_policy)
str(PB_2)
PB_C<- select(PB_2, price, cleaning_fee, guests_included, extra_people, 
              minimum_nights, maximum_nights,number_of_reviews, cancellation_policy)
PB_C$price<- as.numeric(PB_C$price)
PB_C$cleaning_fee<- as.numeric(PB_C$cleaning_fee)
PB_C$extra_people<- as.numeric(PB_C$extra_people)
str(PB_C)

anyNA(PB_C)

#partition data#
set.seed(390)
nrow(PB_C); nrow(PB_C) * 0.6
PB_C1<- sample_n(PB_C, nrow(PB_C))
train<- slice(PB_C1, 1:(nrow(PB_C1)*0.6))
valid<- slice(PB_C1, (nrow(PB_C1)*0.6+1): nrow(PB_C1))

#create neighborhood#
cp<- data.frame(price= as.integer(runif(1, min=min(train$price), max=max(train$price))),
                cleaning_fee= as.integer(runif(1, min=min(train$cleaning_fee), max=max(train$cleaning_fee))),
                guests_included= as.integer(runif(1, min=min(train$guests_included), max=max(train$guests_included))),
                extra_people= as.integer(runif(1, min=min(train$extra_people), max=max(train$extra_people))),
                minimum_nights= as.integer(runif(1, min=min(train$minimum_nights), max=max(train$minimum_nights))),
                maximum_nights= as.integer(runif(1, min=min(train$maximum_nights), max=max(train$maximum_nights))),
                number_of_reviews= as.integer(runif(1, min=min(train$number_of_reviews), max=max(train$number_of_reviews))))
View(cp)
norm.values <- preProcess(train[, 1:7], method=c("center", "scale"))
train[, 1:7] <- predict(norm.values, train[, 1:7])
valid[, 1:7] <- predict(norm.values, valid[, 1:7])
cp <- predict(norm.values, cp)

#find neighborhood#


accuracy<- data.frame(k = seq(1, 14, 1), accuracy = rep(0, 14))
for(i in 1:14) {
  knn.pred <- knn(train[, 1:7], valid[, 1:7],
                  cl = train[, 8], k = i)
  accuracy[i, 2] <- confusionMatrix(knn.pred, valid[, 8])$overall[1]
}
accuracy


ggplot(accuracy, aes(k,accuracy))+geom_point()+
  theme_bw()+
  ggtitle("relationship between k value and accuracy")+
  theme(plot.title=element_text(hjust=0.5),
        axis.text.x=element_text(hjust=1,vjust=1))+
  xlab("k value")+ylab("accuracy")

nn <- knn(train = train[, 1:7], test = cp,
          cl = train[, 8], k = 13)

row.names(train)[attr(nn, "nn.index")]
nn

#second try#
PB_CF<- select(PB_2, price, minimum_nights, maximum_nights, cancellation_policy)
PB_CF$price<- as.numeric(PB_CF$price)
str(PB_CF)

anyNA(PB_CF)

set.seed(390)
nrow(PB_CF); nrow(PB_CF) * 0.6
PB_CF1<- sample_n(PB_CF, nrow(PB_CF))
trainCF<- slice(PB_CF1, 1:(nrow(PB_CF1)*0.6))
validCF<- slice(PB_CF1, (nrow(PB_CF1)*0.6+1): nrow(PB_CF1))

cp2<- data.frame(price= as.integer(runif(1, min=min(trainCF$price), max=max(trainCF$price))),
                 minimum_nights= as.integer(runif(1, min=min(trainCF$minimum_nights), max=max(trainCF$minimum_nights))),
                 maximum_nights= as.integer(runif(1, min=min(trainCF$maximum_nights), max=max(trainCF$maximum_nights))))
View(cp2)
norm.values <- preProcess(trainCF[, 1:3], method=c("center", "scale"))
trainCF[, 1:3] <- predict(norm.values, trainCF[, 1:3])
validCF[, 1:3] <- predict(norm.values, validCF[, 1:3])
cp2 <- predict(norm.values, cp2)

accuracy2<- data.frame(k = seq(1, 14, 1), accuracy2 = rep(0, 14))
for(i in 1:14) {
  knn.pred <- knn(trainCF[, 1:3], validCF[, 1:3],
                  cl = trainCF[, 4], k = i)
  accuracy2[i, 2] <- confusionMatrix(knn.pred, validCF[, 4])$overall[1]
}
accuracy2 #smaller than first try


#third try#
PB_C3<- select(PB_2, id, accommodates, bathrooms, bedrooms, beds, price, cleaning_fee, guests_included, extra_people, 
               minimum_nights, maximum_nights, number_of_reviews, cancellation_policy)
str(PB_C3)
PB_C3$cleaning_fee <- as.numeric(PB_C3$cleaning_fee)
PB_C3$price<- as.numeric(PB_C3$price)
PB_C3$extra_people<- as.numeric(PB_C3$extra_people)

anyNA(PB_C3)
map(PB_C3, ~sum(is.na(.)))
PB_C3<- na.omit(PB_C3)
anyNA(PB_C3)

PB_C3$cancellation_policy<- droplevels(PB_C3$cancellation_policy)
str(PB_C3)

set.seed(390)
nrow(PB_C3); nrow(PB_C3) * 0.6
PB_C31<- sample_n(PB_C3, nrow(PB_C3))
trainC3<- slice(PB_C31, 1:(nrow(PB_C31)*0.6))
validC3<- slice(PB_C31, (nrow(PB_C31)*0.6+1): nrow(PB_C31))

cp3<- data.frame(accommodates= as.integer(runif(1, min=min(trainC3$accommodates), max=max(trainC3$accommodates))),
                 bathrooms= 2.5,
                 bedrooms= as.integer(runif(1, min=min(trainC3$bedrooms), max=max(trainC3$bedrooms))),
                 beds= as.integer(runif(1, min=min(trainC3$beds), max=max(trainC3$beds))),
                 price= as.integer(runif(1, min=min(trainC3$price), max=max(trainC3$price))),
                 cleaning_fee= as.integer(runif(1, min=min(trainC3$cleaning_fee), max=max(trainC3$cleaning_fee))),
                 guests_included= as.integer(runif(1, min=min(trainC3$guests_included), max=max(trainC3$guests_included))),
                 extra_people= as.integer(runif(1, min=min(trainC3$extra_people), max=max(trainC3$extra_people))),
                 minimum_nights= as.integer(runif(1, min=min(trainC3$minimum_nights), max=max(trainC3$minimum_nights))),
                 maximum_nights= as.integer(runif(1, min=min(trainC3$maximum_nights), max=max(trainC3$maximum_nights))),
                 number_of_reviews= as.integer(runif(1, min=min(trainC3$number_of_reviews), max=max(trainC3$number_of_reviews))))
View(cp3)

norm.values <- preProcess(trainC3[, 2:12], method=c("center", "scale"))
trainC3[, 2:12] <- predict(norm.values, trainC3[, 2:12])
validC3[, 2:12] <- predict(norm.values, validC3[, 2:12])
cp3 <- predict(norm.values, cp3)

accuracy3<- data.frame(k = seq(1, 14, 1), accuracy3 = rep(0, 14))
for(i in 1:14) {
  knn.pred <- knn(trainC3[, 2:12], validC3[, 2:12],
                  cl = trainC3[, 13], k = i)
  accuracy3[i, 2] <- confusionMatrix(knn.pred, validC3[, 13])$overall[1]
}
accuracy3

nn <- knn(train = trainC3[, 2:12], test =cp3,
          cl = trainC3[, 13], k = 13)

row.names(trainC3)[attr(nn, "nn.index")]
nn

```


#### B. Write a two-paragraph narrative that describes how you did this. In your narrative, be sure to describe your predictor choices, and mention how you arrived at the particular k value that you used.

Firstly, predictors could be chosen from the original dataset, they are price, cleaning fee, guests included, extra people, minimum nights, maximum nights, number of reviews, cancellation policy. The reason why I choose these factors is that they will affect the cancellation policy. For instance, if the apartment has cleaning fee and the customer want to cancel the booking, the process of cancellation will be complex and maybe involve the benefit of cleaning company. Minimum nights and maximum nights are two factors that will absolutely impact the cancellation policy. If the number of minimum nights that the customer should order is small, such as 1 or 2, and the customer want to cancel the order, the host will not lose too much, so the cancellation policy could be flexible or moderate. On the other hand, if the number of minimum nights is relatively large, such as 10 to 20 days, and the customers want to cancel the order, the host will lose relatively large interest. Guests included and extra people could also be the factor of the model, because it can count the number of customer who want to live in the apartment, if too many people want to live in, the apartment could be large, so if the customer cancel the order, the host could also lose much. But after calculating the accuracy of the model of knn, the value of accuracy is too low, which Is lower than 0.45. Therefore, I change some of the predictors of the model, and thus the second try and third try. In the second try, the accuracy is lower than the first one. Therefore, the third try can be tested, and the accuracy is the highest one, so I decide to choose the third try to be the outcome. The predictors are accommodates, bathrooms, bedrooms, beds, price, cleaning fee, guests included, extra people, minimum nights, maximum nights, number of reviews, cancellation policy, because I believe that the structure of the house can also be the predictor. Then I build up a friction renting called cp3, which is built based on random data, range from the max to the min.
Finally, use the largest value of accuracy to decide the k value, which is k equals to 13, and use the knn model to find the 13 nearest neighborhoods. The prediction outcome shows that the apartment that we built, which is cp3, has strict 14 with grace period policy strategy.

```{r}
row.names(trainC3)[attr(nn, "nn.index")]
nn
```








### Classification, Part II. Naive Bayes

```{r, results = 'hide'}
#Part II. Naive Bayes#
#select the set of predictors#
PBN<- select(PB_2, host_is_superhost, host_has_profile_pic, host_identity_verified, is_location_exact,
             has_availability, requires_license, instant_bookable)
anyNA(PBN)
summary(PBN)

PBNN<- select(PB_2, host_is_superhost, host_has_profile_pic, host_identity_verified, is_location_exact,
             instant_bookable)

PBNN[PBNN ==""]<- "f"
summary(PBNN)
PBNF<- droplevels(PBNN)
summary(PBNF)

#partition data#
set.seed(390)
nrow(PBNF); nrow(PBNF) * 0.6
PBNFF<- sample_n(PBNF, nrow(PBNF))
train1<- slice(PBNFF, 1:(nrow(PBNFF)*0.6))
valid1<- slice(PBNFF, (nrow(PBNFF)*0.6+1): nrow(PBNFF))

#build naive bay#

nbmodel<- naiveBayes(instant_bookable ~., data=train1)
nbmodel

#prove the accuracy#

# training
pred1<- predict(nbmodel, newdata = train1)
confusionMatrix(pred1, train1$instant_bookable)
# validation
pred2 <- predict(nbmodel, newdata = valid1)
confusionMatrix(pred2, valid1$instant_bookable)

#creat fictional apartment#
royal<- data.frame(host_is_superhosts = "t", host_has_profile_pic ="t",
                   host_identity_verified ="f",
                   is_location_exact  ="f" 
)

#make prediction#
answer<- predict(nbmodel,royal)
answer

predict(nbmodel,royal, type="raw")

```

#### A. Using any set of predictors, build a model using the naive Bayes algorithm, with the purpose of predicting whether a particular rental will be instantly bookable.
Response variable: instant_bookable
predictors: host_is_superhost
host_has_profile_pic
host_identity_verified
is_location_exact
has_availability
is_business_travel_ready
requires_license
#### B. Describe a fictional apartment, and use your model to predict which bin it will fall into.
Royal is the fictional apartment built.
#### C. Show a screenshot of the code you used to build your model, the code you used to run the algorithm, and code you used to assess the algorithm.
As shown in starting of Part III.

#### D. Write a two-paragraph narrative that describes how you did this. In your narrative, be sure to talk about things like factor selection and testing against your training data.
On the first try, seven predictors can be chosen to build the model, they are host is superhost, host has profile pic, host identity verified, is location exact, has availability, and requires license. These are the factors that can affect whether the renting is instant bookable. These variables do not contain NA value, but after using summary function, some of them only contain one level, or contain three levels. Therefore, predictors who contain only one level have been dropped, and parameters that have three values have been changed to two levels. The rest variables are host is super host, host has profile pic, host identity verified, is location exact, and instant bookable. In order to test the accuracy of the model, we make a partition of data, dividing to training and validation data set. Next, the naïve bay model has been built, and the response variable is instant bookable. After that, the accuracy test has been made to prove the accuracy of model, and prove that the model can also do well when meeting a new data set. The accuracy of training dataset and validation dataset are 0.7229 and 0.7010 respectively, and the result give us a positive suggestion that the model is accurate, because these two values are similar to each other.
Then a friction apartment has been built, which called royal, and assign the value for different predictors, host is super hosts equals to "t", host has profile pic equals to "t", host identity verified equals to "f", and is location exact equals to "f". The outcome of the prediction is f, so the apartment cannot be instantly bookable. The probability of royal can be or cannot be instantly bookable is 0.6861451 0.3138549 respectively.
```{r}
answer
predict(nbmodel,royal, type="raw")
```










### Classification, Part III. Classification Tree


```{r , results = 'hide'}
#Part III. Classification Tree#

#Classification tree
PB_clf<-PB_2
str(PB_clf)
PB_clf$price<-as.character(PB_clf$price)
PB_clf$cleaning_fee<-as.character(PB_clf$cleaning_fee)
PB_clf$extra_people<-as.character(PB_clf$extra_people)
dollar_cols = c("price","cleaning_fee","extra_people")
for(i in dollar_cols){
  PB_clf[i] = sub("^.","",as.vector(t(PB_clf[i])))
  PB_clf[i] = as.numeric(sub(",","",as.vector(t(PB_clf[i]))))
}
summary(PB_clf$bedrooms)
data_tree<- select(PB_clf,price,cleaning_fee,bedrooms,bathrooms,extra_people)
str(data_tree)
data_tree<-na.omit(data_tree)
summary(data_tree$cleaning_fee)
data_tree$group<-factor(data_tree$cleaning_fee,levels = c("No fee","0-10","10-20","20-30","30-40","40+"))
#Group
x1<-which(data_tree$cleaning_fee==0)
data_tree$group[x1]<-"No fee"
x2<-which(data_tree$cleaning_fee<=10&data_tree$cleaning_fee>0)
data_tree$group[x2]<-"0-10"
x3<-which(data_tree$cleaning_fee<=20&data_tree$cleaning_fee>10)
data_tree$group[x3]<-"10-20"
x4<-which(data_tree$cleaning_fee<=30&data_tree$cleaning_fee>20)
data_tree$group[x4]<-"20-30"
x5<-which(data_tree$cleaning_fee<=40&data_tree$cleaning_fee>30)
data_tree$group[x5]<-"30-40"
x6<-which(data_tree$cleaning_fee>40)
data_tree$group[x6]<-"40+"
str(data_tree$group)
summary(data_tree$group)

#test and train dataset
set.seed(130)
ind<-sample(2,nrow(data_tree),replace=T,prob = c(0.7,0.3))
train<-data_tree[ind==1,]
test<-data_tree[ind==2,]
summary(train)
str(train$square_feet)
set.seed(130)
tree<-rpart(group~price+bathrooms+bedrooms+extra_people,data = train)
```
#### A. Build a classification tree that predicts the size of the cleaning fee that a particular rental will have. Before you can do this, bin the cleaning fees into groups (one of your groups can be “No Fee”).
```{r ,results = 'hide'}
#Group
x1<-which(data_tree$cleaning_fee==0)
data_tree$group[x1]<-"No fee"
x2<-which(data_tree$cleaning_fee<=10&data_tree$cleaning_fee>0)
data_tree$group[x2]<-"0-10"
x3<-which(data_tree$cleaning_fee<=20&data_tree$cleaning_fee>10)
data_tree$group[x3]<-"10-20"
x4<-which(data_tree$cleaning_fee<=30&data_tree$cleaning_fee>20)
data_tree$group[x4]<-"20-30"
x5<-which(data_tree$cleaning_fee<=40&data_tree$cleaning_fee>30)
data_tree$group[x5]<-"30-40"
x6<-which(data_tree$cleaning_fee>40)
data_tree$group[x6]<-"40+"
str(data_tree$group)
summary(data_tree$group)
```


#### B. Determine the ideal size of your tree using cross-validation.

```{r}
#B
tree$cptable
cp<-min(tree$cptable[3,])
prune.tree<-prune(tree,cp=cp)
```
The best cp is 0.135609 for the new model.

#### C. Usingrpart.plotandyourchoiceofgraphicalparameters,showyourtree model here.

```{r}
#C
rpart.plot::rpart.plot(prune.tree)
#cross validation
options(scipen = 999)
a<-printcp(tree)
class(a)
a<- data.frame(a)
which.min(a$xerror)
which.min(a$xstd)
plotcp(tree)
#new model
newtree<-rpart(group~price+bathrooms+bedrooms+extra_people,data = train,xval=3,cp=0.135609)


```

#### D. In a 1-2 paragraph write-up, describe your process. Talk about some of the features that you considered using, and your reasons why. Mention anything that you found interesting as you explored various possible models, and the process you used to arrive at the model you finished with.

We do the data clean firstly. We select the meaningless variables in the step 1 and then let them group a new data group. Delete this group from raw dataset. We remove some columns that are all NA, or replace some NA with other values. For example, I replace the NA of cleaning fee row with mean number. When I establish the model, I think the price, number of bathrooms, number of bedrooms and extra people will influence the cleaning fee, so I choose them to be the variables for predicting the size of the cleaning fee. And then I category the cleaning fee with different classes to group a new row.	
The model from rpart choose price to be the variable. We predict that when the price is larger than seventy seven, the cleaning fee belong to 40+ group, when the price is smaller than 72 and larger than 41, the cleaning fee belong to 20-30 group, when the price is smaller than 41, the cleaning fee is belong to 10-20 group by the classification tree. The chart shows us the classification tree below. And the best cp is 0.135609.













## Step IV: Clustering (15 points)
### I. Perform either a k-means analysis or a hierarchical clustering analysis in order to place rentals within your neighborhood into clusters.
** Of any section of the project, this one offers the most opportunity to be creative and take risks. Think about feature engineering, too--how/when/wherecanyou create new variables based on existing ones?



#### Data Preparation for Clustering

```{r,results = 'hide'}
PB_cluster = PB_3
#PB_cluster <- read.csv("PB_cluster.csv")
#View(PB_cluster)

```

**1. Drop useless features**
```{r,results = 'hide'}
PB_cluster = subset(PB_cluster,select =- c(id,name,summary,description,neighborhood_overview,notes,host_verifications,
                                latitude,longitude,transit,access,interaction,house_rules,
                                host_id,host_name,host_about,amenities))
```




**2. Missing Value**



square_feet
```{r,results = 'hide'}
#summary(PB_cluster$square_feet)  # there is too many missing value, therefore, I delete it in 3.(2) part.
PB_cluster$square_feet[is.na(PB_cluster$square_feet) == T] <- "N/A"
PB_cluster$square_feet <-as.factor(PB_cluster$square_feet)
```

security_deposit
I assumed security_deposit is 0 if there is no information about security deposit
```{r}
#summary(PB_cluster$security_deposit)
PB_cluster$security_deposit[is.na(PB_cluster$security_deposit)] <- 0

```

cleaning_fee. I assumed cleaning_fee. is 0 if there is no information about the fee
```{r}
#summary(PB_cluster$cleaning_fee)
PB_cluster$cleaning_fee[is.na(PB_cluster$cleaning_fee)] <- 0
```



reviews
```{r}
# review_scores_rating, review_scores_accuracy, review_scores_cleanliness, review_scores_checkin, review_scores_communication, review_scores_location,  review_scores_value

# no_review_data = PB_cluster[is.na(PB_cluster$review_scores_accuracy) == T,]
review_cols = c("review_scores_rating", "review_scores_accuracy", "review_scores_cleanliness", "review_scores_checkin", "review_scores_communication", "review_scores_location",  "review_scores_value")
for (i in review_cols){
        PB_cluster[i][is.na(PB_cluster[i]) == T] = median(PB_cluster[,i],na.rm = T)
}

# reviews_per_month
PB_cluster$reviews_per_month[is.na(PB_cluster$reviews_per_month) == T] = 0
#summary(PB_cluster$reviews_per_month)

PB_cluster$first_review  = as.factor(PB_cluster$first_review)
#summary(PB_cluster)

```

host_response_rate
```{r}
response_rate = as.numeric(sub("%","",PB_2$host_response_rate))/100
response_rate[is.na(response_rate) == T] = median(response_rate,na.rm = T)
PB_cluster$host_response_rate = response_rate
```



**2. Feature Engineering**
first_review      last_review    
* Here, we created a new feature Business_interval, which measures number of day after this airbnb has first review. 
* Also, we created a feature named No_Business_interval. This measures number of day after the last review. Longer interval means this airbnb is no business recently. 
* The third feature is operation_interval, which is number of day after host_since. This measure how long that the airbnb is.
There are missing value in first_review and last_review the two variables. The reason would be there is no review. Therefore, the time interval would be 0.
```{r}

first_review = as.Date(PB_2$first_review,"%Y-%m-%d")
last_review = as.Date(PB_2$last_review,"%Y-%m-%d")
host_since = as.Date(PB_2$host_since,"%Y-%m-%d")
Business_interval = today() - first_review
No_Business_interval = today() - last_review
operation_interval = today() - host_since

Business_interval[is.na(Business_interval) == T] = 0
No_Business_interval[is.na(No_Business_interval) == T] = 0 
operation_interval[is.na(operation_interval) == T] = 0 

PB_cluster$Business_interval = as.numeric(Business_interval)
PB_cluster$No_Business_interval = as.numeric(No_Business_interval)
PB_cluster$operation_interval = as.numeric(operation_interval)


```


calendar_updated
Same as Prediction part, we transformed the categorical variables into numeric variables as the day after last updated. For example, "a week ago" will be replaced by 7. 
```{r}
PB_cluster$calendar_updated = PB_lm$calendar_updated
```


weekly_price and monthly_price

The feature engineering process is same as what I did in prediction part. I filled the missing value of weekly_price and monthly_price using the daily price * the number of day(7 for week and 30 for month). And calculate the discount_ratio, which is equal to weekly_price dividing by (daily price times number of day). This feature can measure how much discount that this airbnb offer.
The only difference with **Step II Prediction** is that, in predictions, we measured whether offering discount or not.
```{r}
# weekly_price

#summary(PB_cluster$weekly_price)
for (i in 1:length(PB_cluster$weekly_price)) {
        if (is.na(PB_cluster$weekly_price[i]) == T){
                PB_cluster$weekly_price[i] = PB_cluster$price[i]*7
        }
}

weekly_discount_ratio = (PB_cluster$weekly_price)/(PB_cluster$price*7)
weekly_discount_ratio[is.na(weekly_discount_ratio) == T] = 1
#summary(weekly_discount_ratio)
PB_cluster$weekly_discount_ratio = weekly_discount_ratio

# monthly_price
#summary(PB_cluster$monthly_price)
for (i in 1:length(PB_cluster$monthly_price)) {
        if (is.na(PB_cluster$monthly_price[i]) == T){
                PB_cluster$monthly_price[i] = PB_cluster$price[i]*30
        }
}

monthly_discount_ratio = (PB_cluster$monthly_price)/(PB_cluster$price*30)
monthly_discount_ratio[is.na(monthly_discount_ratio) == T] = 1
#summary(monthly_discount_ratio)
PB_cluster$monthly_discount_ratio = monthly_discount_ratio
```



**outlier analysis** 
K-means and hierarchical clustering analysis are sensitive with outlier. According to the analysis in **Step II Prediction**. I delete 3 point which the price is extremely large (>1000). 
```{r}
PB_cluster = PB_cluster[PB_cluster$price <1000,]
```







#### K-means modeling
This dataset is used for K-means, I keep all the numeric variables.
```{r}
PB_kmeans = select(PB_cluster, c(host_response_rate,host_listings_count , host_total_listings_count,
                      accommodates, bathrooms ,bedrooms,beds,price,security_deposit,  cleaning_fee,
                   guests_included,   extra_people,     minimum_nights,     maximum_nights,
                   calendar_updated ,availability_30,  availability_60, availability_90, availability_365,
                   number_of_reviews,review_scores_rating, review_scores_accuracy ,review_scores_cleanliness,
                   review_scores_checkin,review_scores_communication, review_scores_location, review_scores_value,
                   calculated_host_listings_count, reviews_per_month, Business_interval, No_Business_interval,
                   operation_interval,weekly_discount_ratio, monthly_discount_ratio))
```


```{r,results = 'hide'}
PB_kmeans.norm <- sapply(PB_kmeans, scale)
summary(PB_kmeans)
```


```{r}
#Elbow Method for finding the optimal number of clusters
set.seed(123)
# Compute and plot wss for k = 1 to k = 10.
k.max <- 10
data <- PB_kmeans.norm
wss <- sapply(1:k.max, 
              function(k){mean(kmeans(data, k)$withinss)})
wss
plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Average Within−Cluster Squared Distance",
     main = "COMPARING DIFFERENT CHOICES OF k IN TERMS OF OVERALL AVERAGE WITHIN-CLUSTER DISTANCE",
     cex.main=0.8,cex.lab=0.7, cex.axis=0.8)

```


```{r}
set.seed(123)
km <- kmeans(PB_kmeans.norm, 4)
Clusters = as.data.frame(km$centers)
Clusters = cbind(Clusters,count = km$size)
Clusters
```


### II. Show your code and results, and name each of your clusters. In 1-2 paragraphs, describe the process that you used.

Process analysis

To get the k-mean, numeric data are required to put into the model. However, some numeric data contain different level of missing value, and we noticed those NA will affect our model and outcome. Therefore, we chose different way to handle those NA according to the number of missing values in each variable. We deleted entire square feet (column) as 95% are NA. For the rest of numeric data, instead of removing those NA directly, we replaced those NA with 0 or median to maintain the number of data and reduce the bias. We also took fully advantage of each variable by converting them to a way we can use it in our k-mean model, like using today() – host_since (column) to get the number of days for each host in his or her Airbnb business. Furthermore, we also created several new variables by converting some unrelated data to useful data, like weekly and monthly discount ratio. It allows us to increase the number of features we can use for feature engineering and clustering. 


Based on the elbow chart, it shows the graph start flatting after 4th point, indicating k = 4 is an ideal k-mean in this case, so that the housing style will be separated to 4 clusters based on its rentals price.


Cluster 1: seasonal holiday rental
It shows a significant feature in its weekly and monthly rental discount rate, 20% and 30% respectively, those value indicates a preference of long-term rental. As a result, it causes the lowest daily price when compare to the rest of three clusters. Meanwhile, it also shows the lowest accommodation capacity, like number of bedrooms, beds and bathrooms, representing limit amount of housing capacity. On the other side, it shows the highest value in almost all of its review scores, such as rating, accuracy, cleanliness, check-in, communication and location, those value indicate a good customer feedback and high-quality service provided by host. According to those features, this cluster can be classified as seasonal holiday rental. 

Cluster 2: private room rental
The strongest feature of this cluster is its price and customer review levels are quite close to 0, which is the average of each variable. Compare to cluster 1 and 3, cluster 2 requires relatively high deposit and cleaning fee, it is like a normal pay of Airbnb rental service. But its number of reviews and reviews per month stand on top of four clusters, indicating it has a quite stable demand form the market. Meanwhile, it shows the lowest weekly and monthly discount, which indicates houses are most likely focus on daily rental, so it could be classified as private room rental. 

Cluster 3: shared room rental
This cluster also shows the second lowest value in price, -0.25, sounds like economy rental class compare to cluster 2 and 4. One of significant features is its review scores are all negative, indicating a relatively low customers experience. Those situations could happen in shared room as customers only pay for each single bed, so it is cheap to customers, but sharing rooms with other somewhat will bring a negative impact to customers experience. however, its cheap price makes it attractive to those who don’t have a big budget in accommodation, such as backpacker or some sort of travelers. 

Cluster 4: entire house rental 
It shows the highest value in number in accommodation, beds, bathrooms, bedrooms as well price, cleaning fee and deposit, strongly indicate renting a place with huge amount of space. Based on those features, it is very likely renting an entire house or apartment for big group size customers like family or community. 


